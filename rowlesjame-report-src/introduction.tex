\paragraph{Acknowledgements}
I want to thank my supervisors Sharon Gao and Peter Gu for their support and guidance throughout this project. I would also like to express my gratitude towards my supervisors for providing me with this opportunity. As an individual who highly values the pursuit of learning and has played both roles - teacher and student. I feel very privileged to be given the opportunity to contribute back to this community. Lastly, I would like to thank my examiners for their contribution to completing this project.

\backmatter

\chapter{Introduction}\label{C:intro}

\section{Context}

Hundreds of students take the English Proficiency Programme (EPP) every year. Its aim is to help students develop their English in a welcoming learning setting. In addition, the EPP can train a pupil for more English-language academic training. The EPP facilitates a test to assess a student's ability to see if they would meet the entry criteria for their specific undergraduate or postgraduate course. This test consists of multiple components, including a multi-choice and a writing component. Based on how they perform overall dictates whether or not they will enter their proposed course [1]. However, if their overall performance is unsatisfactory, they will stay with the EEP to build their skills. In this case, it is beneficial for the academic staff of the EPP to understand which areas the student is struggling in to provide more tailored support for that student. 

\section{Problem}

The problem that exists here is that manually marking the writing component of this test takes too long and, as a result, is not included when making the final decision regarding whether or not a student has met their respective course's entry criteria. Excluding this component means that students are potentially not assessed as accurately as they could be. As a result,they can not receive the most optimal academic experience.

\section{Purpose}

The purpose of this project is to create a system that can automate this task.
Roughly, this looks like a system that can use an essay as input and, in return, provide quantifiable metrics regarding the essay's use of the English written language. 

\section{Goals}

The goals for the solution are for it to be relatively fast, precise enough, and easy to use and understand. 
\paragraph{Speed}
As speed is a leading contributor to the problem, this plays a massive part in the goals of the solution. Specifically, this means that it must be faster than if a human marker were doing it. 

\paragraph{Accuracy}
We need the system to predict similar marks to what a human rater might produce in terms of accuracy. However, as mentioned previously, the writing component is not used currently due to time constraints. As a result, even if the system were roughly accurate, it would still provide valuable insight to the academic staff. For example, its results could be used "with a grain of salt" and in combination with other assessment metrics at the final-judgment stage of the EPP program.

\paragraph{Easy of Use}
In terms of ease of use, for the final product, this may be defined as having a polished user experience held together with simple, maintainable network architecture. For this project's scope, I want to prioritize keeping the system explainable or understandable so that non-technical stakeholders have insight into the nature of the system - how it works, why it does things. Additionally, I want the system to be easily understood by a technical stakeholder such as the next student to work on this project. 

\section{Approach}

Initially, we saw there being two approaches to the problem. 

\paragraph{Approach 1} The first approach would provide a binary output to indicate whether the student had a good enough proficiency to enter their proposed university course or needed to continue with the EPP to improve their English writing proficiency further.

\paragraph{Approach 2}
The second approach would aim to go a step further by providing a continuous output that would provide insight into the extent of a student's overall writing proficiency and their categorical proficiency in areas such as; grammar, vocab, or ideas. With this approach, not only do we receive enough information to assess if they can enter their proposed university course or not. We can also go a step further by having insight into the extent to which they may or may not do so. For example, if the student needs to get an overall score of 4 to progress and receive a 3, this might lead us to believe that maybe they are not ready to progress. Also, with this approach, we get the same continuous-type output for specific categories. This means that, for the students who do not progress, we have insight into the areas that a student may be struggling with and the extent to which they may be struggling with them. This, then, allows for more tailed academic support. The same goes for areas that a student might be proficient in. 

\paragraph{}The latter provides the most information and consequently the most benefit for all stakeholders. So right from the offset, this was the approach that I wanted to take, provided it was a feasible pursuit.