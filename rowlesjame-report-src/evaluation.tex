\chapter{Evaluation}\label{C:eval}

\section{The Feature Extractor}
Within The Feature Extractor, simply identifying a feature is a major breakthrough. However, this is a time consuming and complex process. Beyond this, the next most valuable task that can be is understanding if a feature is in the correct Feature Category. However, due to the nature of feature engineering, as it solely relies on domain knowledge, there is likely always room for further optimisation or refinement, beyond the initial discovery or identification of them. As a result, it is worthwhile to investigate to what extent a feature is actually aiding the predictive model to verify our assumptions of the domain and potentially optimise our system. 

\paragraph{Feature Importance Tests}
Feature Importance Tests can be useful for this. Feature Importance Tests refer to techniques that assign a score to input features based on how useful they are at predicting a target variable [x]. These give us an indication into whether or not our features are helping the predictor to synthesize a relationship between themselves and the score. 

The results can then be used to delete the features with the lowest scores or keep the features with the highest scores. This can simplify the problem that is being modeled, speed up the modeling process, and potentially, improve the performance of the model [x]. 

\paragraph{F-Value}
The values or scores of each feature are generated through an F-Test. The F-Test is a form of hypothesis testing. Where a model is created with just a constant and another model is created with a constant and a feature. Then the least squared errors in both the models are compared to checks if the difference in errors between each model is significant or introduced by chance [x].

Below are the features for each category that have the highest importance.
- I will compare the actual highest to the features that are currently being used.
- 

\subsubsection{Vocab}

Here we can see that 13 of the top 38 features are currently being used to predicting the Vocab score. This is $34\%$ of this range. However, 8 of the top 19 are being used to predict the Vocab score. Which is an improvement, at $50\%$. 

This is one of the higher proportions of strong features being used. However, the  vocab category has the most amount of features, excluding the overall category, due to it consisting of the headwords feature type. As the way this group of features is calculated is by producing a feature for each band of the headwords. Because of this, this would lead me to believe why there are a lot scores in the first two thirds of the tables.

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Feature &
  Score &
  &
  Feature &
  Score &
  &
  Feature &
  Score \\ \midrule
\cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abandon{]}\_headwords} &
  \cellcolor[HTML]{EFEFEF}\textbf{22.05833729} &
  &
  nfunction\_tokens &
  5.704050657 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abomasum{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.095114627} \\
\cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}accent{]}\_headwords} &
  \cellcolor[HTML]{EFEFEF}\textbf{16.40796885} &
  &
  ncontent\_types &
  5.219737871 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abomasum{]}\_headwords} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.095114627} \\
polysyllabcount &
  15.71130959 &
  &
  n\_trigram\_lemma\_types &
  5.118016095 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_sentences} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.078611375} \\
\cellcolor[HTML]{EFEFEF}\textbf{difficult\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{12.18352548} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{num\_types} &
  \cellcolor[HTML]{EFEFEF}\textbf{5.118016095} &
  &
  ncontent\_tokens &
  3.07783032 \\
determiners\_total &
  11.54240886 &
  &
  nlemma\_types &
  5.118016095 &
  &
  ts\_avg\_len\_sent &
  3.002896576 \\
smog\_index &
  9.174520901 &
  &
  n\_bigram\_lemma\_types &
  5.118016095 &
  &
  automated\_readability\_index &
  2.779641652 \\
determiners\_per\_words &
  8.446785067 &
  &
  crawford &
  4.81807535 &
  &
  monosyllabcount &
  2.616700091 \\
text\_standard &
  7.983828856 &
  &
  lexicon\_count &
  4.468261165 &
  &
  conjunctions\_total &
  2.568940715 \\
syllable\_count &
  7.092957374 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}aardvark{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.44829349} &
  &
  nfunction\_types &
  2.476570719 \\
\cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abaft{]}\_headwords} &
  \cellcolor[HTML]{EFEFEF}\textbf{6.930334973} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}aardvark{]}\_headwords} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.44829349} &
  &
  pct\_transitions &
  2.444242565 \\
\cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abaft{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{6.895066765} &
  &
  nlemmas &
  4.300215352 &
  &
  spache\_readability &
  2.440962422 \\
\cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abandon{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{6.703801959} &
  &
  n\_trigram\_lemmas &
  4.300215352 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{words\_div\_sentences} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.37079476} \\
\cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}absentminded{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{6.424550614} &
  &
  n\_bigram\_lemmas &
  4.300215352 &
  &
  flesch\_kincaid\_grade &
  2.231981694 \\
\cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}accent{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{6.358088016} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}a{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.274963711} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abashed{]}\_total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.144350221} \\
function\_ttr &
  6.283303783 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.246082192} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}abashed{]}\_headwords} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.144350221} \\
grammar\_errors\_ltp\_by\_words &
  5.873555877 &
  &
  pct\_rel\_trigrams &
  4.07939033 &
  &
  coleman\_liau\_index &
  1.978447928 \\
letter\_count &
  5.856224883 &
  &
  pronouns\_total &
  3.865049524 &
  &
  pronouns\_noun\_ratio &
  1.765296516 \\
reading\_time &
  5.742222651 &
  &
  ts\_avg\_syllab\_per\_word &
  3.349429602 &
  &
  pronouns\_density &
  1.74673426 \\
char\_count &
  5.742099891 &
  &
  sentence\_count &
  3.194236862 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{proportion\_vocab\_list{[}a{]}\_headwords} &
  \cellcolor[HTML]{EFEFEF}\textbf{1.739721638} \\ \bottomrule
\end{tabular}%
}
\caption{Vocab Feature Importance Tests}
\label{tab:e1}
\end{table}
% ==========================================

\subsubsection{Ideas}

Here we can see that 2 of the top 38 features are currently being used to predict the Vocab score. This is $5\%$ of this range, which is very low. Additionally, 1 of the top 19 are being used to predict the Vocab score. Which is also $5\%$.

The vocab category has the most amount of features, excluding the overall category, due to it consisting of the headwords feature type. As the way this group of features is calculated is by producing a feature for each band of the headwords. Because of this, this would lead me to believe why there are a lot scores in the first two thirds of the tables.

These results are potentially some of the most valuable. As this category was and the most neglected due to its complexity. The marking criteria definition for the high end scores of the Ideas category is "Development of ideas is deep and convincing." For this reason, the category only consisted of the "miscellaneous" features which were added to all feature categories. These include: total words total sentences and sentence density. 

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Feature &
  Score &
  &
  Feature &
  Score &
  &
  Feature &
  Score \\ \midrule
determiners\_total &
  23.98975321 &
  &
  monosyllabcount &
  17.06445867 &
  &
  pronouns\_density &
  2.86945213 \\
syllable\_count &
  22.65767743 &
  &
  polysyllabcount &
  16.01149404 &
  &
  content\_ttr &
  2.796727653 \\
letter\_count &
  21.76693064 &
  &
  proportion\_vocab\_list{[}a{]}\_headwords &
  15.05858081 &
  &
  ttr\_nouns &
  2.708399971 \\
reading\_time &
  21.50866321 &
  &
  nouns\_total &
  14.93762526 &
  &
  conjunctions\_unique &
  2.399994321 \\
char\_count &
  21.50722445 &
  &
  ncontent\_types &
  14.48729494 &
  &
  pronouns\_unique &
  2.399994321 \\
nfunction\_tokens &
  21.03021095 &
  &
  pct\_transitions &
  14.19203036 &
  &
  ts\_avg\_sent\_per\_word &
  2.38213524 \\
nfunction\_types &
  20.2273507 &
  &
  proportion\_vocab\_list{[}a{]}\_total\_words &
  12.68306971 &
  &
  proportion\_vocab\_list{[}abomasum{]}\_total\_words &
  2.296469714 \\
function\_ttr &
  20.22092973 &
  &
  pronouns\_total &
  11.80391532 &
  &
  proportion\_vocab\_list{[}abomasum{]}\_headwords &
  2.296469714 \\
lexicon\_count &
  20.04425343 &
  &
  difficult\_words &
  11.43922304 &
  &
  proportion\_vocab\_list{[}aal{]}\_total\_words &
  1.736032636 \\
nlemmas &
  19.87833192 &
  &
  conjunctions\_total &
  9.925480901 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{sent\_density} &
  \cellcolor[HTML]{EFEFEF}\textbf{1.494882434} \\
n\_bigram\_lemmas &
  19.87833192 &
  &
  proportion\_vocab\_list{[}accent{]}\_headwords &
  9.86627013 &
  &
  proportion\_vocab\_list{[}a1{]}\_total\_words &
  1.231983429 \\
n\_trigram\_lemmas &
  19.87833192 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_sentences} &
  \cellcolor[HTML]{EFEFEF}\textbf{9.683989634} &
  &
  pronouns\_noun\_ratio &
  1.22992392 \\
\cellcolor[HTML]{EFEFEF}\textbf{total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{19.58377946} &
  &
  sentence\_count &
  9.665452001 &
  &
  text\_standard &
  1.229178768 \\
pct\_rel\_trigrams &
  18.84225246 &
  &
  ttrr &
  9.10015428 &
  &
  linsear\_write\_formula &
  1.195434814 \\
ncontent\_tokens &
  17.70236467 &
  &
  unique\_words &
  9.080102356 &
  &
  proportion\_vocab\_list{[}absentminded{]}\_headwords &
  1.191540578 \\
n\_trigram\_lemma\_types &
  17.21977037 &
  &
  nouns\_unique &
  7.210491094 &
  &
  proportion\_vocab\_list{[}abalone{]}\_total\_words &
  1.10443711 \\
num\_types &
  17.21977037 &
  &
  proportion\_vocab\_list{[}abandon{]}\_headwords &
  4.724206672 &
  &
  proportion\_vocab\_list{[}abalone{]}\_headwords &
  1.10443711 \\
nlemma\_types &
  17.21977037 &
  &
  determiners\_per\_words &
  4.698053181 &
  &
  dale\_chall\_readability\_score &
  1.059783983 \\
n\_bigram\_lemma\_types &
  17.21977037 &
  &
  proportion\_vocab\_list{[}aal{]}\_headwords &
  4.37518797 &
  &
  proportion\_vocab\_list{[}abandon{]}\_total\_words &
  0.7944183939 \\ \bottomrule
\end{tabular}%
}
\caption{Ideas Feature Importance Tests}
\label{tab:e2}
\end{table}
% ==========================================

\subsubsection{Grammar}

Here we can see that 5 of the top 38 features are currently being used to predicting the Grammar score. This is $13\%$ of the most important features for this range. For the next range, we see similar results as 3 of the top 19 are being used to predict the Grammar score. Which is a slight at $15\%$.

This feature only consisted of three features although I was very confident in the grammar errors feature as I assumed this would be very strongly related. This assumption has (lucky) proven correct in the results as it is the strongest feature. Additionally, the remaining grammar-unique features from this category are within the top 19 features. 

However, despite the fact that the  current features for this category are important. There simply is just enough of them. 

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Feature &
  Score &
  &
  Feature &
  Score &
  &
  Feature &
  Score \\ \midrule
\cellcolor[HTML]{EFEFEF}\textbf{grammar\_errors\_ltp\_by\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{13.86887276} &
  &
  n\_trigram\_lemmas &
  8.428847883 &
  &
  proportion\_vocab\_list{[}abomasum{]}\_total\_words &
  3.447080979 \\
polysyllabcount &
  13.5453572 &
  &
  proportion\_vocab\_list{[}accent{]}\_headwords &
  8.335462779 &
  &
  proportion\_vocab\_list{[}abomasum{]}\_headwords &
  3.447080979 \\
determiners\_total &
  11.99318654 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{8.332141187} &
  &
  smog\_index &
  2.652814039 \\
syllable\_count &
  11.06434199 &
  &
  ncontent\_types &
  7.590394235 &
  &
  proportion\_vocab\_list{[}abnormal{]}\_total\_words &
  2.578601176 \\
letter\_count &
  9.777308962 &
  &
  ncontent\_tokens &
  7.000690642 &
  &
  proportion\_vocab\_list{[}aback{]}\_headwords &
  2.451993818 \\
nfunction\_tokens &
  9.676747576 &
  &
  proportion\_vocab\_list{[}a{]}\_headwords &
  6.970460707 &
  &
  proportion\_vocab\_list{[}aback{]}\_total\_words &
  2.412191877 \\
reading\_time &
  9.589911019 &
  &
  monosyllabcount &
  6.821351862 &
  &
  proportion\_vocab\_list{[}abaft{]}\_total\_words &
  2.159653449 \\
char\_count &
  9.587821097 &
  &
  proportion\_vocab\_list{[}abnormal{]}\_headwords &
  6.794462298 &
  &
  proportion\_vocab\_list{[}abaft{]}\_headwords &
  2.037732545 \\
difficult\_words &
  9.085364454 &
  &
  function\_ttr &
  5.868971179 &
  &
  nouns\_unique &
  2.03145763 \\
\cellcolor[HTML]{EFEFEF}\textbf{pct\_rel\_trigrams} &
  \cellcolor[HTML]{EFEFEF}\textbf{8.874333893} &
  &
  proportion\_vocab\_list{[}abandon{]}\_headwords &
  5.325501914 &
  &
  proportion\_vocab\_list{[}absentminded{]}\_total\_words &
  2.008426737 \\
nfunction\_types &
  8.78489224 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_sentences} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.777733853} &
  &
  proportion\_vocab\_list{[}abashed{]}\_total\_words &
  1.807824726 \\
\cellcolor[HTML]{EFEFEF}\textbf{pct\_transitions} &
  \cellcolor[HTML]{EFEFEF}\textbf{8.683182431} &
  &
  sentence\_count &
  4.665500367 &
  &
  proportion\_vocab\_list{[}abashed{]}\_headwords &
  1.807824726 \\
lexicon\_count &
  8.642680781 &
  &
  pronouns\_total &
  3.991921505 &
  &
  text\_standard &
  1.679480615 \\
n\_trigram\_lemma\_types &
  8.633375961 &
  &
  determiners\_per\_words &
  3.877961209 &
  &
  ttrr &
  1.666647575 \\
num\_types &
  8.633375961 &
  &
  conjunctions\_total &
  3.595950896 &
  &
  crawford &
  1.66445437 \\
nlemma\_types &
  8.633375961 &
  &
  proportion\_vocab\_list{[}a{]}\_total\_words &
  3.561214109 &
  &
  unique\_words &
  1.496118802 \\
n\_bigram\_lemma\_types &
  8.633375961 &
  &
  nouns\_total &
  3.448940441 &
  &
  linsear\_write\_formula &
  1.48898339 \\
nlemmas &
  8.428847883 &
  &
  proportion\_vocab\_list{[}abet{]}\_total\_words &
  3.447080979 &
  &
  proportion\_vocab\_list{[}a1{]}\_total\_words &
  1.389620477 \\
n\_bigram\_lemmas &
  8.428847883 &
  &
  proportion\_vocab\_list{[}abet{]}\_headwords &
  3.447080979 &
  &
  ts\_avg\_sent\_per\_word &
  0.7677552826 \\ \bottomrule
\end{tabular}%
}
\caption{Grammar Feature Importance Tests}
\label{tab:e3}
\end{table}
% ==========================================

\subsubsection{Flow}

Here we can see that 10 of the top 38 features are currently being used to predict the Flow score. This is $26\%$ of the most important features for this range. For the next range, we see the exact same results as 5 of the top 19 are being used to predict the Grammar score. Which is $26\%$ of this range.

This feature consists of a huge amount of imported libraries that calculate readability measures for the text. For this reason, I find it quite surprising that there is not a greater percentage for these measures with these ranges. Five of the twelve readability measures are evident in the top 38 features.

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Feature &
  Score &
  &
  Feature &
  Score &
  &
  Feature &
  Score \\ \midrule
grammar\_errors\_ltp\_by\_words &
  10.52322172 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{reading\_time} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.446741393} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{linsear\_write\_formula} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.179626868} \\
proportion\_vocab\_list{[}accent{]}\_headwords &
  8.616507797 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{char\_count} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.445241765} &
  &
  proportion\_vocab\_list{[}a{]}\_total\_words &
  1.970600376 \\
proportion\_vocab\_list{[}abaft{]}\_headwords &
  7.174452719 &
  &
  nfunction\_types &
  3.25497433 &
  &
  proportion\_vocab\_list{[}aback{]}\_total\_words &
  1.786283529 \\
proportion\_vocab\_list{[}abaft{]}\_total\_words &
  7.137848797 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{crawford} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.153343606} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{monosyllabcount} &
  \cellcolor[HTML]{EFEFEF}\textbf{1.743636837} \\
pct\_transitions &
  5.952052458 &
  &
  proportion\_vocab\_list{[}abet{]}\_total\_words &
  2.785493827 &
  &
  proportion\_vocab\_list{[}aal{]}\_total\_words &
  1.605845889 \\
\cellcolor[HTML]{EFEFEF}\textbf{polysyllabcount} &
  \cellcolor[HTML]{EFEFEF}\textbf{5.908466276} &
  &
  proportion\_vocab\_list{[}abet{]}\_headwords &
  2.785493827 &
  &
  proportion\_vocab\_list{[}abrasion{]}\_headwords &
  1.539322698 \\
difficult\_words &
  5.813492969 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{lexicon\_count} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.775661365} &
  &
  pronouns\_total &
  1.371066736 \\
\cellcolor[HTML]{EFEFEF}\textbf{text\_standard} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.888289364} &
  &
  proportion\_vocab\_list{[}a{]}\_headwords &
  2.771527007 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{coleman\_liau\_index} &
  \cellcolor[HTML]{EFEFEF}\textbf{1.332429938} \\
proportion\_vocab\_list{[}abandon{]}\_headwords &
  4.575020547 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.733166972} &
  &
  proportion\_vocab\_list{[}aback{]}\_headwords &
  1.320345559 \\
n\_trigram\_lemma\_types &
  4.263709123 &
  &
  nlemmas &
  2.66E+00 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_sentences} &
  \cellcolor[HTML]{EFEFEF}\textbf{1.119213959} \\
num\_types &
  4.263709123 &
  &
  n\_bigram\_lemmas &
  2.661185087 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{sentence\_count} &
  \cellcolor[HTML]{EFEFEF}\textbf{1.118883069} \\
nlemma\_types &
  4.263709123 &
  &
  n\_trigram\_lemmas &
  2.661185087 &
  &
  proportion\_vocab\_list{[}aardvark{]}\_total\_words &
  0.9790015297 \\
n\_bigram\_lemma\_types &
  4.263709123 &
  &
  ncontent\_tokens &
  2.62501352 &
  &
  proportion\_vocab\_list{[}aardvark{]}\_headwords &
  0.9790015297 \\
\cellcolor[HTML]{EFEFEF}\textbf{syllable\_count} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.156071732} &
  &
  determiners\_total &
  2.558186665 &
  &
  sent\_density &
  0.9545624739 \\
pct\_rel\_trigrams &
  4.011612142 &
  &
  nfunction\_tokens &
  2.530773085 &
  &
  proportion\_vocab\_list{[}abate{]}\_total\_words &
  0.9144704138 \\
ncontent\_types &
  4.011139066 &
  &
  proportion\_vocab\_list{[}absentminded{]}\_total\_words &
  2.46882231 &
  &
  proportion\_vocab\_list{[}abnormal{]}\_headwords &
  0.90326088 \\
function\_ttr &
  3.796428186 &
  &
  proportion\_vocab\_list{[}accent{]}\_total\_words &
  2.23868793 &
  &
  proportion\_vocab\_list{[}abate{]}\_headwords &
  0.8364237459 \\
\cellcolor[HTML]{EFEFEF}\textbf{letter\_count} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.528072315} &
  &
  proportion\_vocab\_list{[}abashed{]}\_total\_words &
  2.215948697 &
  &
  nouns\_total &
  0.8331892199 \\
\cellcolor[HTML]{EFEFEF}\textbf{smog\_index} &
  \cellcolor[HTML]{EFEFEF}\textbf{3.499672392} &
  &
  proportion\_vocab\_list{[}abashed{]}\_headwords &
  2.215948697 &
  &
  conjunctions\_unique &
  0.8321611094 \\ \bottomrule
\end{tabular}%
}
\caption{Flow Feature Importance Tests}
\end{table}
% ==========================================

\subsubsection{Coherence}

Here we can see that 16 of the top 38 features are currently being used to predict the Coherence score. This is $42\%$ of this range, which is the highest percentage of all of categories for this range (excluding the overall category). Additionally, 8 of the top 19 are being used to predict the Coherence score. Which is the again, the highest percentage of all of the categories for this range (excluding the overall category), at $50\%$. 

This score has the features with the highest percentage of important features in both bands. However, it does have the second largest amount of features, excluding the overall category. As the way this group of features is calculated is by producing a feature for each band of the headwords. Because of this, this would lead me to believe why there are a lot scores in the first two thirds of the tables.

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\cmidrule(r){1-2} \cmidrule(lr){4-5} \cmidrule(l){7-8}
Feature &
  Score &
  &
  Feature &
  Score &
  &
  Feature &
  Score \\ \cmidrule(r){1-2} \cmidrule(lr){4-5} \cmidrule(l){7-8} 
pct\_transitions &
  20.68884211 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{pronouns\_total} &
  \cellcolor[HTML]{EFEFEF}\textbf{8.5764678} &
  &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{pronouns\_density}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{3.860901774}} \\
polysyllabcount &
  17.13412183 &
  &
  monosyllabcount &
  8.14921556 &
  &
  proportion\_vocab\_list{[}a1{]}\_total\_words &
  3.355255868 \\
pct\_rel\_trigrams &
  14.92687473 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{n\_trigram\_lemma\_types} &
  \cellcolor[HTML]{EFEFEF}\textbf{7.57892787} &
  &
  crawford &
  3.083923886 \\
syllable\_count &
  13.17692848 &
  &
  num\_types &
  7.57892787 &
  &
  proportion\_vocab\_list{[}abnormal{]}\_headwords &
  3.059688495 \\
\cellcolor[HTML]{EFEFEF}\textbf{nfunction\_tokens} &
  \cellcolor[HTML]{EFEFEF}\textbf{11.94914458} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{nlemma\_types} &
  \cellcolor[HTML]{EFEFEF}\textbf{7.57892787} &
  &
  sentence\_count &
  2.858495234 \\
letter\_count &
  11.73462468 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{n\_bigram\_lemma\_types} &
  \cellcolor[HTML]{EFEFEF}\textbf{7.57892787} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{total\_sentences} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.780063655} \\
proportion\_vocab\_list{[}abandon{]}\_headwords &
  11.47026469 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{ncontent\_tokens} &
  \cellcolor[HTML]{EFEFEF}\textbf{7.555855104} &
  &
  proportion\_vocab\_list{[}absentminded{]}\_total\_words &
  2.685789236 \\
reading\_time &
  11.40182309 &
  &
  proportion\_vocab\_list{[}a{]}\_total\_words &
  7.412367857 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{ttr\_nouns} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.41010852} \\
char\_count &
  11.39842839 &
  &
  difficult\_words &
  6.882954381 &
  &
  ts\_avg\_sent\_per\_word &
  2.248255026 \\
\cellcolor[HTML]{EFEFEF}\textbf{function\_ttr} &
  \cellcolor[HTML]{EFEFEF}\textbf{10.84975839} &
  &
  proportion\_vocab\_list{[}aback{]}\_headwords &
  6.814253694 &
  &
  proportion\_vocab\_list{[}abet{]}\_total\_words &
  2.182106948 \\
grammar\_errors\_ltp\_by\_words &
  10.77253679 &
  &
  proportion\_vocab\_list{[}a{]}\_headwords &
  6.698095193 &
  &
  proportion\_vocab\_list{[}abet{]}\_headwords &
  2.182106948 \\
lexicon\_count &
  10.08744734 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{ncontent\_types} &
  \cellcolor[HTML]{EFEFEF}\textbf{6.48683046} &
  &
  proportion\_vocab\_list{[}abomasum{]}\_total\_words &
  2.182106948 \\
\cellcolor[HTML]{EFEFEF}\textbf{determiners\_total} &
  \cellcolor[HTML]{EFEFEF}\textbf{9.994962745} &
  &
  text\_standard &
  5.44173295 &
  &
  proportion\_vocab\_list{[}abomasum{]}\_headwords &
  2.182106948 \\
\cellcolor[HTML]{EFEFEF}\textbf{nlemmas} &
  \cellcolor[HTML]{EFEFEF}\textbf{9.723691954} &
  &
  proportion\_vocab\_list{[}accent{]}\_headwords &
  4.88851034 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{pronouns\_noun\_ratio} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.106469914} \\
\cellcolor[HTML]{EFEFEF}\textbf{n\_trigram\_lemmas} &
  \cellcolor[HTML]{EFEFEF}\textbf{9.723691954} &
  &
  smog\_index &
  4.881499435 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{sent\_density} &
  \cellcolor[HTML]{EFEFEF}\textbf{2.048119793} \\
\cellcolor[HTML]{EFEFEF}\textbf{n\_bigram\_lemmas} &
  \cellcolor[HTML]{EFEFEF}\textbf{9.723691954} &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{conjunctions\_total} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.850229186} &
  &
  proportion\_vocab\_list{[}abate{]}\_total\_words &
  1.722627897 \\
\cellcolor[HTML]{EFEFEF}\textbf{total\_words} &
  \cellcolor[HTML]{EFEFEF}\textbf{9.591339672} &
  &
  ttrr &
  4.744063714 &
  &
  proportion\_vocab\_list{[}abate{]}\_headwords &
  1.696145125 \\
proportion\_vocab\_list{[}aback{]}\_total\_words &
  8.807317784 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{nouns\_total} &
  \cellcolor[HTML]{EFEFEF}\textbf{4.63369171} &
  &
  proportion\_vocab\_list{[}a1{]}\_headwords &
  1.614302704 \\
\cellcolor[HTML]{EFEFEF}\textbf{nfunction\_types} &
  \cellcolor[HTML]{EFEFEF}\textbf{8.672600025} &
  &
  unique\_words &
  4.609612529 &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{nouns\_unique} &
  \cellcolor[HTML]{EFEFEF}\textbf{1.429434557} \\ \bottomrule
\end{tabular}%
}
\caption{Coherence Feature Importance Tests}
\label{tab:e4}
\end{table}
% ==========================================

\subsubsection{Overall}

What's the overall category and am yeah I can't simply categories performances for you don't insights into the features it's it's using versus sun features and it's not using because consists of all features however you that I see here is in narrowing down and the features for this category for you yeah as in at the moment it is using all 57 of them and and and maybe more only 19.

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Feature &
  Score &
  &
  Feature &
  Score &
  &
  Feature &
  Score \\ \midrule
proportion\_vocab\_list{[}abandon{]}\_headwords &
  14.27582377 &
  &
  lexicon\_count &
  6.93800963 &
  &
  proportion\_vocab\_list{[}abet{]}\_total\_words &
  2.77008194 \\
difficult\_words &
  13.41906313 &
  &
  total\_words &
  6.834380155 &
  &
  proportion\_vocab\_list{[}abet{]}\_headwords &
  2.77008194 \\
polysyllabcount &
  13.29707246 &
  &
  nlemmas &
  6.773600167 &
  &
  proportion\_vocab\_list{[}abomasum{]}\_total\_words &
  2.77008194 \\
proportion\_vocab\_list{[}accent{]}\_headwords &
  11.3781753 &
  &
  n\_bigram\_lemmas &
  6.773600167 &
  &
  proportion\_vocab\_list{[}abomasum{]}\_headwords &
  2.77008194 \\
grammar\_errors\_ltp\_by\_words &
  10.02287274 &
  &
  n\_trigram\_lemmas &
  6.773600167 &
  &
  proportion\_vocab\_list{[}abandon{]}\_total\_words &
  2.560321881 \\
syllable\_count &
  9.680129981 &
  &
  proportion\_vocab\_list{[}abaft{]}\_headwords &
  6.175245252 &
  &
  determiners\_per\_words &
  2.497674073 \\
pct\_transitions &
  9.2517999 &
  &
  proportion\_vocab\_list{[}abaft{]}\_total\_words &
  6.144068368 &
  &
  nouns\_total &
  2.348175159 \\
n\_trigram\_lemma\_types &
  8.952429235 &
  &
  ncontent\_tokens &
  5.601640629 &
  &
  nouns\_unique &
  2.180390661 \\
num\_types &
  8.952429235 &
  &
  function\_ttr &
  5.590956532 &
  &
  proportion\_vocab\_list{[}abnormal{]}\_total\_words &
  2.177297632 \\
nlemma\_types &
  8.952429235 &
  &
  proportion\_vocab\_list{[}abnormal{]}\_headwords &
  5.37E+00 &
  &
  proportion\_vocab\_list{[}absentminded{]}\_total\_words &
  2.163955982 \\
n\_bigram\_lemma\_types &
  8.952429235 &
  &
  proportion\_vocab\_list{[}a{]}\_headwords &
  5.094092462 &
  &
  conjunctions\_total &
  1.943824923 \\
determiners\_total &
  8.946634884 &
  &
  pronouns\_total &
  4.941964937 &
  &
  proportion\_vocab\_list{[}accent{]}\_total\_words &
  1.924996303 \\
letter\_count &
  8.360611133 &
  &
  monosyllabcount &
  4.86820757 &
  &
  proportion\_vocab\_list{[}abashed{]}\_total\_words &
  1.921267615 \\
ncontent\_types &
  8.261155741 &
  &
  total\_sentences &
  4.242184438 &
  &
  proportion\_vocab\_list{[}abashed{]}\_headwords &
  1.921267615 \\
reading\_time &
  8.192861291 &
  &
  sentence\_count &
  4.196859193 &
  &
  proportion\_vocab\_list{[}aback{]}\_total\_words &
  1.477536266 \\
char\_count &
  8.191934694 &
  &
  proportion\_vocab\_list{[}a{]}\_total\_words &
  4.152176495 &
  &
  pronouns\_density &
  1.201514767 \\
nfunction\_tokens &
  7.81515932 &
  &
  crawford &
  3.163807878 &
  &
  proportion\_vocab\_list{[}abate{]}\_total\_words &
  1.083243997 \\
pct\_rel\_trigrams &
  7.552931616 &
  &
  text\_standard &
  3.063653346 &
  &
  proportion\_vocab\_list{[}aback{]}\_headwords &
  1.052754839 \\
nfunction\_types &
  7.307528473 &
  &
  smog\_index &
  3.025845124 &
  &
  linsear\_write\_formula &
  1.04153577 \\ \bottomrule
\end{tabular}%
}
\caption{Overall Feature Importance Tests}
\label{tab:e5}
\end{table}
% ==========================================

\section{The Predictor}

To experiment with The Predictor’s understanding of the relationship between an essay's features and their respective computer-generated scores. I have explored multiple models, in conjunction with multiple datasets, and performed a Train-Test Evaluation for each. My motivation for these was to not only observe the performance of the predictor for the domain data. But also to gain insight into its relative performance to other systems, and observe its robotness with similar data. 

\paragraph{Test Train Split}
This process is used to evaluate the performance of a machine learning algorithm. It requires data as an input which demonstrates the relationship in which we’re trying to synthesize [x]. So in our case, this data is the numerical representations of the features from real world essays along with their respective human-given scores from the EPP. Then this data is divided into two subsets. The first is provided to the model for it to configure it’s understanding of this relationship. The second is used to test it’s understanding of this relationship. These sets must be different because the underlying purpose of this type of investigation - to observe a model's ability to predict data that it hasn’t seen before [x]. Based on it’s base-nature combined with it’s configured understanding of this relationship which has been refined using the train dataset. Once the test dataset has been run through the model, we can then simply compare the model's predicted scores with the actual scores to produce a number of metrics to explain the model's generalized prediction performance. 

\paragraph{Considerations} In general, this procedure is appropriate when the dataset is a suitable representation of the domain and is an appropriate size. If both of these criteria are not met, less optimal results can occur. The main concern with the Hewlett Datasets is their ability to accurately represent the domain problem. This is discussed in the datasets section. In terms of the VUW dataset, it is on the small side. The optimal size for a machine learning algorithm can vary based on the problem. It can require thousands or millions of examples. What I’ve observed in other AES studies is between 1000-2000 essays [17]. However the EPP data only consists of 113 essays. This is a concern as it can mean that the data from this set can be poorly descriptive of the relationship under investigation. As it may not exhibit a representative distribution of the common and uncommon cases that make up the general relationship of the data. Which can mean that the model can receive insufficient resources to configure an effective understanding between the inputs and outputs. This can also mean that the error measures can be overly accurate [x]. A suitable alternative procedure in this case is the k-fold cross-validation [x]. However this was not done in this study. The error measures are the specific calculations that we use to interpret the performance of the model. 

\paragraph{Agreement Tests}
In statistics, inter-rater reliability (also called by various similar names, such as inter-rater agreement, inter-rater concordance, inter-observer reliability, and so on) is the degree of agreement among independent observers who rate, code, or assess the same phenomenon. In this I propose four Agreement Measures measures to analyze the results; Quadratic Weighted Kappa (QWK), Pearson’s Correlation (P), Adjacent Agreement (A),  percentage, Exact Agreement percentage (E)

\paragraph{Quadratic Weighted Kappa} Quadratic Weighted Kappa measures the agreement percentage between the machine and human after correcting for the likelihood that some agreement between raters might happen by random chance. Quadratic Weighted Kappa is considered to be the best  measure of agreement in the AES field because of the chance agreement correction [18]. 
The QWK score is a ratio between -1 and 1. A negative QWK score implies that the agreement is "worse than random". A random model should give a score of close to 0. A perfect predictions will give a score of 1.0 [19]. Below are some guide lines for this metric proposed by Viera & Garrett [20].  Additionally, the ETS uses a QWK value of 0.70 as an established baseline for the assessment of the level of agreement between a human and an AES system. Anything less than this threshold is considered as a bad agreement and anything above it is considered as a good agreement[15].

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
QWK       & Agreement Representation \\ \midrule
0.01–0.20 & slight agreement         \\
0.21–0.40 & fair agreement           \\
0.41–0.60 & moderate agreement       \\
0.61–0.80 & substantial agreement    \\
0.81–0.99 & almost perfect agreement \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Pearson’s Correlation} Pearson’s correlation is the product-moment correlation between the machine-produced predicted score and the human marker final score. It ranges from -1 to +1. A higher positive correlation indicates stronger agreement. 

\paragraph{Adjacent Agreement Percentage} Adjacent-agreement percentage refers to the agreement between scores (i.e., machine and human) that are within one point of one another, as a percentage. A value of 1.0 is perfect agreement and 0.0 is no agreement. 

\paragraph{Exact Agreement Percentage} Exact-agreement percentage refers to the agreement between two scores, as a percentage. A value of 1.0 is perfect agreement and 0.0 is no agreement.

\subsection{Overall}

\subsection{VUW EPP}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllllllllllll@{}}
\cmidrule(r){1-7} \cmidrule(l){9-15}
Score &
  Category &
  QWK &
  P &
  A &
  E &
  Model &
  &
  Score &
  Category &
  QWK &
  P &
  A &
  E &
  Model \\ \cmidrule(r){1-7} \cmidrule(l){9-15} 
\cellcolor[HTML]{EFEFEF}\textbf{Grammar} &
  \cellcolor[HTML]{EFEFEF}\textbf{Grammar} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.295} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.312} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.957} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.652} &
  \cellcolor[HTML]{EFEFEF}\textbf{RFR} &
  &
  Ideas &
  Grammar &
  0.42 &
  0.527 &
  1 &
  0.478 &
  SVR \\
Grammar &
  Vocab &
  0.253 &
  0.265 &
  0.913 &
  0.478 &
  DTR &
  &
  Ideas &
  Vocab &
  0.41 &
  0.417 &
  0.957 &
  0.478 &
  DTR \\
Grammar &
  Flow &
  0.359 &
  0.362 &
  1 &
  0.522 &
  KR &
  &
  Ideas &
  Flow &
  0.386 &
  0.507 &
  1 &
  0.435 &
  SVR \\
Grammar &
  Ideas &
  0.064 &
  0.07 &
  0.87 &
  0.348 &
  DTR &
  &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{Ideas}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{Ideas}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{0.42}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{0.527}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{1}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{0.478}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{SVR}} \\
Grammar &
  Coherence &
  0.233 &
  0.258 &
  0.957 &
  0.522 &
  GBR &
  &
  Ideas &
  Coherence &
  0.383 &
  0.451 &
  0.957 &
  0.522 &
  LR \\
Grammar &
  Overall &
  0.19 &
  0.211 &
  0.957 &
  0.565 &
  EN &
  &
  Ideas &
  Overall &
  0.386 &
  0.507 &
  1 &
  0.435 &
  SVR \\
Vocab &
  Grammar &
  0.299 &
  0.406 &
  0.957 &
  0.609 &
  GBR &
  &
  Coherence &
  Grammar &
  0.265 &
  0.352 &
  0.957 &
  0.522 &
  SVR \\
\cellcolor[HTML]{EFEFEF}\textbf{Vocab} &
  \cellcolor[HTML]{EFEFEF}\textbf{Vocab} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.476} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.544} &
  \cellcolor[HTML]{EFEFEF}\textbf{1} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.609} &
  \cellcolor[HTML]{EFEFEF}\textbf{GBR} &
  &
  Coherence &
  Vocab &
  0.43 &
  0.442 &
  0.957 &
  0.565 &
  DTR \\
Vocab &
  Flow &
  0.343 &
  0.359 &
  1 &
  0.565 &
  KR &
  &
  Coherence &
  Flow &
  0.426 &
  0.436 &
  0.957 &
  0.652 &
  DTR \\
Vocab &
  Ideas &
  0.189 &
  0.35 &
  0.957 &
  0.435 &
  LR &
  &
  Coherence &
  Ideas &
  0.265 &
  0.352 &
  0.957 &
  0.522 &
  SVR \\
Vocab &
  Coherence &
  0.343 &
  0.467 &
  0.957 &
  0.565 &
  LR &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{Coherence} &
  \cellcolor[HTML]{EFEFEF}\textbf{Coherence} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.419} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.515} &
  \cellcolor[HTML]{EFEFEF}\textbf{1} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.565} &
  \cellcolor[HTML]{EFEFEF}\textbf{SVR} \\
Vocab &
  Overall &
  0.499 &
  0.509 &
  1 &
  0.609 &
  DTR &
  &
  Coherence &
  Overall &
  0.332 &
  0.361 &
  1 &
  0.522 &
  KR \\
Flow &
  Grammar &
  0.373 &
  0.5 &
  1 &
  0.435 &
  LR &
  &
  Overall &
  Grammar &
  0.359 &
  0.422 &
  1 &
  0.522 &
  RFR \\
Flow &
  Vocab &
  0.389 &
  0.398 &
  1 &
  0.478 &
  DTR &
  &
  Overall &
  Vocab &
  0.515 &
  0.517 &
  1 &
  0.565 &
  DTR \\
\cellcolor[HTML]{EFEFEF}\textbf{Flow} &
  \cellcolor[HTML]{EFEFEF}\textbf{Flow} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.405} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.41} &
  \cellcolor[HTML]{EFEFEF}\textbf{1} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.522} &
  \cellcolor[HTML]{EFEFEF}\textbf{DTR} &
  &
  Overall &
  Flow &
  0.437 &
  0.443 &
  1 &
  0.478 &
  DTR \\
Flow &
  Ideas &
  0.361 &
  0.361 &
  1 &
  0.565 &
  DTR &
  &
  Overall &
  Ideas &
  0.337 &
  0.427 &
  1 &
  0.391 &
  GBR \\
Flow &
  Coherence &
  0.28 &
  0.289 &
  0.957 &
  0.478 &
  DTR &
  &
  Overall &
  Coherence &
  0.405 &
  0.516 &
  1 &
  0.522 &
  LR \\
Flow &
  Overall &
  0.287 &
  0.339 &
  1 &
  0.522 &
  KR &
  &
  \cellcolor[HTML]{EFEFEF}\textbf{Overall} &
  \cellcolor[HTML]{EFEFEF}\textbf{Overall} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.535} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.562} &
  \cellcolor[HTML]{EFEFEF}\textbf{1} &
  \cellcolor[HTML]{EFEFEF}\textbf{0.652} &
  \cellcolor[HTML]{EFEFEF}\textbf{KR} \\ \bottomrule
\end{tabular}%
}
\caption{Categorical $\&$ Overall Agreement Measures}
\end{table}

\subsubsection{Categorical}

For the Grammar category, the QWK was 0.295 which represents “fair agreement” between the machine predicted essay score and the human marker essay score. The correlation between the machine predicted score and the human-produced score was 0.312 which indicates a weak correlation. The adjacent agreement was 0.957 meaning that the machine and human score were within one point of one another $95\%$ of the time. Exact agreement between the machine-predicted score and the human-produced score occurred $65\%$ of the time. The highest scoring model was the RFR.

For the Vocab category, the QWK was 0.476 which represents “moderate agreement” between the machine predicted essay score and the human marker essay score. The correlation between the machine predicted score and the human-produced score was 0.544 which indicates a moderate correlation. The adjacent agreement was 1 meaning that the machine and human score were within one point of one another $100\%$ of the time. Exact agreement between the machine-predicted score and the human-produced score occurred $60\%$ of the time. The highest scoring model was the GBR.

For the Flow category, the QWK was 0.405 which represents “fair agreement” between the machine predicted essay score and the human marker essay score. The correlation between the machine predicted score and the human-produced score was 0.41 which indicates a moderate correlation. The adjacent agreement was1 meaning that the machine and human score were within one point of one another $100\%$ of the time. Exact agreement between the machine-predicted score and the human-produced score occurred $52\%$ of the time. The highest scoring model was the DTR.

For the Ideas category, the QWK was 0.42 which represents “moderate agreement” between the machine predicted essay score and the human marker essay score. The correlation between the machine predicted score and the human-produced score was 0.527 which indicates a moderate correlation. The adjacent agreement was 1 meaning that the machine and human score were within one point of one another $100\%$ of the time. Exact agreement between the machine-predicted score and the human-produced score occurred $47\%$ of the time. The highest scoring model was the SVR.

\subsubsection{Overall}

For the Overall category, the QWK was 0.535 which represents “moderate agreement” between the machine predicted essay score and the human marker essay score. The correlation between the machine predicted score and the human-produced score was 0.562 which indicates a moderate correlation. The adjacent agreement was 1 meaning that the machine and human score were within one point of one another $100\%$ of the time. Exact agreement between the machine-predicted score and the human-produced score occurred $65\%$ of the time. The highest scoring model was the KR.

\subsection{Hewlett}

\subsubsection{Overall}

% ==========================================
\begin{table}[h]
\centering{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Dataset & Score   & Category & QWK   & P     & A     & E     & Model \\ \midrule
1       & Overall & Overall  & 0.798 & 0.84  & 0.899 & 0.459 & RFR   \\
3       & Overall & Overall  & 0.546 & 0.654 & 0.977 & 0.471 & BR    \\
4       & Overall & Overall  & 0.634 & 0.744 & 0.977 & 0.46  & SVR   \\
5       & Overall & Overall  & 0.735 & 0.736 & 0.967 & 0.587 & DTR   \\
6       & Overall & Overall  & 0.647 & 0.739 & 0.969 & 0.458 & LR    \\
7       & Overall & Overall  & 0.747 & 0.761 & 0.392 & 0.131 & KR    \\
8       & Overall & Overall  & 0.711 & 0.753 & 0.393 & 0.152 & GBR   \\ \bottomrule
\end{tabular}%
}
\caption{trtgfhfgh}
\end{table}
% ==========================================

For the Overall Scores for all Hewlett datasets, the QWK was between 0.546-798. For all most of all of these, the value was above 0.634  which represents “substantial agreement” between the machine predicted essay score and the human marker essay score. The correlation between the machine predicted score and the human-produced score was mostly within the 0.7 range which indicates a strong correlation. The adjacent agreement was very close to 1 for all datasets except for the last two meaning that the machine and human score were within one point of one another almost $100\%$ of the time. For the datasets which did not have as measure here. This is likely due to their different scoring ranges. As the overall score ranges for these datasets are ... and ... . Exact agreement between the machine-predicted score and the human-produced score occurred approximately on $50\%$ of the time for all datasets except the last two. Again, I believe this is due to their score range differences. The highest scoring models were quite varied: RFR, BR, SVR, DTR, LR, KR, GBR.

\subsubsection{Categorical}

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\centering
\begin{tabular}{@{}lllllll@{}}
\toprule
Score                 & Category  & QWK   & P     & A     & E     & Model \\ \midrule
Writing\_Applications & Grammar   & 0.565 & 0.684 & 0.986 & 0.489 & SVR   \\
Writing\_Applications & Vocab     & 0.583 & 0.585 & 0.986 & 0.581 & DTR   \\
Writing\_Applications & Flow      & 0.596 & 0.6   & 0.981 & 0.567 & DTR   \\
Writing\_Applications & Ideas     & 0.567 & 0.687 & 0.986 & 0.483 & SVR   \\
Writing\_Applications & Coherence & 0.558 & 0.56  & 0.975 & 0.586 & DTR   \\
Writing\_Applications & Overall   & 0.617 & 0.621 & 0.983 & 0.597 & DTR   \\
Language\_Conventions & Grammar   & 0.626 & 0.629 & 0.983 & 0.653 & DTR   \\
Language\_Conventions & Vocab     & 0.544 & 0.549 & 0.975 & 0.561 & DTR   \\
Language\_Conventions & Flow      & 0.482 & 0.488 & 0.975 & 0.519 & DTR   \\
Language\_Conventions & Ideas     & 0.342 & 0.529 & 0.969 & 0.347 & SVR   \\
Language\_Conventions & Coherence & 0.404 & 0.409 & 0.95  & 0.528 & DTR   \\
Language\_Conventions & Overall   & 0.566 & 0.571 & 0.983 & 0.586 & DTR   \\ \bottomrule
\end{tabular}
\caption{Hewlett Set \#2 Agreement Measures}
\end{table}
% ==========================================

ll

% ==========================================
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\centering
\begin{tabular}{@{}lllllll@{}}
\toprule
Score        & Category  & QWK   & P     & A     & E     & Model \\ \midrule
Overall      & Grammar   & 0.696 & 0.713 & 0.341 & 0.118 & RFR   \\
Overall      & Vocab     & 0.719 & 0.741 & 0.341 & 0.118 & GBR   \\
Overall      & Flow      & 0.708 & 0.725 & 0.369 & 0.127 & GBR   \\
Overall      & Ideas     & 0.706 & 0.726 & 0.347 & 0.124 & GBR   \\
Overall      & Coherence & 0.747 & 0.761 & 0.366 & 0.14  & LR    \\
Overall      & Overall   & 0.747 & 0.761 & 0.392 & 0.131 & KR    \\
Ideas        & Grammar   & 0.427 & 0.429 & 0.908 & 0.468 & DTR   \\
Ideas        & Vocab     & 0.427 & 0.545 & 0.952 & 0.341 & LR    \\
Ideas        & Flow      & 0.425 & 0.559 & 0.946 & 0.35  & GBR   \\
Ideas        & Ideas     & 0.425 & 0.505 & 0.962 & 0.36  & SVR   \\
Ideas        & Coherence & 0.46  & 0.552 & 0.968 & 0.398 & SVR   \\
Ideas        & Overall   & 0.475 & 0.607 & 0.939 & 0.369 & LR    \\
Organization & Grammar   & 0.44  & 0.441 & 0.968 & 0.564 & DTR   \\
Organization & Vocab     & 0.43  & 0.43  & 0.965 & 0.545 & DTR   \\
Organization & Flow      & 0.445 & 0.445 & 0.959 & 0.567 & DTR   \\
Organization & Ideas     & 0.491 & 0.491 & 0.981 & 0.554 & DTR   \\
Organization & Coherence & 0.466 & 0.596 & 0.984 & 0.465 & BR    \\
Organization & Overall   & 0.456 & 0.587 & 0.984 & 0.452 & KR    \\
Style        & Grammar   & 0.458 & 0.459 & 0.987 & 0.637 & DTR   \\
Style        & Vocab     & 0.474 & 0.476 & 0.994 & 0.64  & DTR   \\
Style        & Flow      & 0.421 & 0.422 & 0.987 & 0.627 & DTR   \\
Style        & Ideas     & 0.507 & 0.508 & 1.0   & 0.643 & DTR   \\
Style        & Coherence & 0.479 & 0.48  & 0.997 & 0.621 & DTR   \\
Style        & Overall   & 0.469 & 0.47  & 0.997 & 0.624 & DTR   \\
Conventions  & Grammar   & 0.417 & 0.418 & 0.968 & 0.599 & DTR   \\
Conventions  & Vocab     & 0.407 & 0.408 & 0.981 & 0.567 & DTR   \\
Conventions  & Flow      & 0.375 & 0.376 & 0.975 & 0.541 & DTR   \\
Conventions  & Ideas     & 0.392 & 0.393 & 0.975 & 0.567 & DTR   \\
Conventions  & Coherence & 0.439 & 0.439 & 0.971 & 0.618 & DTR   \\
Conventions  & Overall   & 0.418 & 0.541 & 0.994 & 0.522 & BR    \\ \bottomrule
\end{tabular}
\caption{Hewlett Set \#7 Agreement Measures}
\end{table}
% ==========================================

For the Ideas category, the QWK was 0.425 which represents “moderate agreement” between the machine predicted essay score and the human marker essay score. The correlation between the machine predicted score and the human-produced score was 0.505 which indicates a moderate correlation. The adjacent agreement was very close to one meaning that the machine and human score were within one point of one another almost $100\%$ of the time. Exact agreement between the machine-predicted score and the human-produced score occurred $36\%$ of the time. The highest scoring model was the SVR.
These measures are not that strong although this is to be expected as the ideas category was the most underdeveloped feature category. It only consists of three very simple features.



% ==========================================
\begin{table}[h]
\centering
\begin{tabular}{lllllll}
\toprule
Score               & Category  & QWK   & P     & A     & E     & Model \\ \midrule
Overall             & Grammar   & 0.657 & 0.702 & 0.338 & 0.124 & GBR   \\
Overall             & Vocab     & 0.689 & 0.724 & 0.324 & 0.152 & GBR   \\
Overall             & Flow      & 0.618 & 0.648 & 0.29  & 0.09  & RFR   \\
Overall             & Ideas     & 0.541 & 0.574 & 0.228 & 0.076 & GBR   \\
Overall             & Coherence & 0.656 & 0.694 & 0.29  & 0.09  & LR    \\
Overall             & Overall   & 0.711 & 0.753 & 0.393 & 0.152 & GBR   \\
Ideas\_And\_Content & Grammar   & 0.439 & 0.539 & 0.972 & 0.566 & SVR   \\
Ideas\_And\_Content & Vocab     & 0.432 & 0.603 & 0.979 & 0.455 & RFR   \\
Ideas\_And\_Content & Flow      & 0.454 & 0.568 & 0.979 & 0.559 & SVR   \\
Ideas\_And\_Content & Ideas     & 0.443 & 0.54  & 0.972 & 0.572 & SVR   \\
Ideas\_And\_Content & Coherence & 0.428 & 0.566 & 0.972 & 0.503 & RFR   \\
Ideas\_And\_Content & Overall   & 0.493 & 0.495 & 0.979 & 0.552 & DTR   \\
Organization        & Grammar   & 0.485 & 0.58  & 0.993 & 0.566 & SVR   \\
Organization        & Vocab     & 0.528 & 0.529 & 0.979 & 0.662 & DTR   \\
Organization        & Flow      & 0.503 & 0.611 & 0.993 & 0.572 & SVR   \\
Organization        & Ideas     & 0.493 & 0.583 & 0.993 & 0.579 & SVR   \\
Organization        & Coherence & 0.45  & 0.451 & 0.986 & 0.586 & DTR   \\
Organization        & Overall   & 0.479 & 0.603 & 0.986 & 0.552 & SVR   \\
Voice               & Grammar   & 0.372 & 0.515 & 0.972 & 0.593 & LR    \\
Voice               & Vocab     & 0.456 & 0.461 & 0.986 & 0.607 & DTR   \\
Voice               & Flow      & 0.385 & 0.526 & 0.979 & 0.628 & SVR   \\
Voice               & Ideas     & 0.369 & 0.504 & 0.979 & 0.628 & SVR   \\
Voice               & Coherence & 0.402 & 0.404 & 0.972 & 0.621 & DTR   \\
Voice               & Overall   & 0.411 & 0.412 & 0.979 & 0.607 & DTR   \\
Word\_Choice        & Grammar   & 0.451 & 0.461 & 1.0   & 0.634 & DTR   \\
Word\_Choice        & Vocab     & 0.41  & 0.415 & 0.993 & 0.69  & DTR   \\
Word\_Choice        & Flow      & 0.37  & 0.479 & 1.0   & 0.6   & SVR   \\
Word\_Choice        & Ideas     & 0.373 & 0.48  & 1.0   & 0.607 & SVR   \\
Word\_Choice        & Coherence & 0.346 & 0.513 & 0.993 & 0.497 & SVR   \\
Word\_Choice        & Overall   & 0.409 & 0.424 & 0.993 & 0.669 & DTR   \\
Sentence\_Fluency   & Grammar   & 0.377 & 0.461 & 0.993 & 0.572 & SVR   \\
Sentence\_Fluency   & Vocab     & 0.416 & 0.424 & 0.972 & 0.634 & DTR   \\
Sentence\_Fluency   & Flow      & 0.366 & 0.462 & 0.986 & 0.545 & SVR   \\
Sentence\_Fluency   & Ideas     & 0.377 & 0.461 & 0.993 & 0.572 & SVR   \\
Sentence\_Fluency   & Coherence & 0.342 & 0.53  & 0.979 & 0.421 & SVR   \\
Sentence\_Fluency   & Overall   & 0.37  & 0.377 & 0.979 & 0.566 & DTR   \\
Conventions         & Grammar   & 0.348 & 0.348 & 0.993 & 0.503 & DTR   \\
Conventions         & Vocab     & 0.379 & 0.384 & 0.979 & 0.517 & DTR   \\
Conventions         & Flow      & 0.35  & 0.364 & 0.972 & 0.503 & DTR   \\
Conventions         & Ideas     & 0.376 & 0.381 & 0.979 & 0.497 & DTR   \\
Conventions         & Coherence & 0.404 & 0.406 & 0.993 & 0.552 & DTR   \\
Conventions         & Overall   & 0.411 & 0.422 & 0.979 & 0.641 & DTR  \\ \bottomrule
\end{tabular}
\caption{Hewlett Set \#8 Agreement Measures}
\end{table}
% ==========================================






